{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pix2Pix with Tensorflow 2.0 and Kubeflow Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**WIP/TODO**\n",
    "\n",
    "- Add link + copyright to Pix2Pix homepage + Tensorflow 2.0 Tutorials\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Requirements\n",
    "\n",
    "**WIP/TODO**\n",
    "\n",
    "- Setup a Kubeflow Cluster\n",
    "    - Easiest : Deploy Kubeflow on GKE : Cloud Deploy \n",
    "    - Customize installation (Add NFS Storage, Google Storage, GPU node pool, \"gcloud services enable ')\n",
    "       - GPU require a Google Account with Billing Enabled\n",
    "       - Get 300$ of FREE credits\n",
    "    - Start a Jupyter Server on Kubeflow\n",
    "    - Clone this project repository in your Jupyter Notebook environment\n",
    "\n",
    "\n",
    "- Learn more on Kubeflow...\n",
    "- Learn more on Tensorflow 2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What you will learn\n",
    "\n",
    "**WIP/TODO** \n",
    "- Use Kubelow to create a Deep Learning Pipeline for Tensorflow 2.0 (using GPU)\n",
    "- Learn to create Kubeflow Pipelines Components\n",
    "- Define and execute a Kubelow pipelines from this Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Word on Pix2Pix and the Training Pipeline we will define below\n",
    "\n",
    "**WIP/TODO**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get ready for the execution of this Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------\n",
    "#           IMPORTANT : \n",
    "#      Customize this variable with \n",
    "#        your own GCP Project ID\n",
    "#  (will be used by Google Cloud Build)\n",
    "# -------------------------------------\n",
    "\n",
    "\n",
    "GCP_PROJECT_ID=None\n",
    "\n",
    "assert (GCP_PROJECT_ID) != None, \"Your must set your own GCP Project ID\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------\n",
    "#    Notebook configuration 'magic'\n",
    "# -------------------------------------\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%config InlineBackend.figure_format = 'retina'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If necessary install the latest Kubeflow Pipelines SDK. This Python library allows to define a pipeline composed of multiple tasks, and execute it on a Kubeflow Cluster "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# -------------------------------------\n",
    "#   Install Kubeflow Pipelines SDK\n",
    "# -------------------------------------\n",
    "\n",
    "!pip3 install https://storage.googleapis.com/ml-pipeline/release/0.1.20/kfp.tar.gz \\\n",
    "              --quiet --disable-pip-version-check --upgrade\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------\n",
    "#     Import Kubeflow Pipelines SDK \n",
    "# -------------------------------------\n",
    "\n",
    "import kfp\n",
    "import kfp.dsl as dsl\n",
    "import kfp.gcp as gcp\n",
    "import kfp.notebook\n",
    "import kfp.components as comp\n",
    "from kfp import compiler\n",
    "from kubernetes import client as k8s_client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------\n",
    "#          Import PiX2Pix code \n",
    "# -------------------------------------\n",
    "\n",
    "from download_dataset import *\n",
    "from prepare_dataset import *\n",
    "#from train_pix2pix import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------\n",
    "#    DEFINE SOME PROJECT VARIABLES\n",
    "# -------------------------------------\n",
    "URL = \"https://people.eecs.berkeley.edu/~tinghuiz/projects/pix2pix/datasets/facades.tar.gz\"\n",
    "FILE_NAME = \"facades.tar.gz\"\n",
    "\n",
    "NFS_MOUNT = \"/mnt/nfs\"\n",
    "KERAS_CACHE_DIR = \"/mnt/nfs/data/\"\n",
    "\n",
    "TRAIN_IMG_SUBDIR=\"train/\"\n",
    "TEST_IMG_SUBDIR=\"test/\" \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learn to Create Kubeflow Pipelines Components "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 3 ways of using the Pix2Pix source code:\n",
    "\n",
    "-  First, **without using Kubeflow**, we can normaly use the python code from download_dataset.py as for any other python code. For example:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "````\n",
    "# -------------------------------------\n",
    "#       Local Python execution \n",
    "#           (No Kubeflow)\n",
    "# -------------------------------------\n",
    "\n",
    "!python download_dataset.py --fname facades.tar.gz \\ \n",
    "--origin \"https://people.eecs.berkeley.edu/~tinghuiz/projects/pix2pix/datasets/facades.tar.gz\"\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can also create a **Kubeflow pipeline component**, by simply converting a Python function into a Kubeflow Pipelines Operation, as in the code example below:\n",
    "\n",
    "   (**However, make sure** that all the python libraries that will be used are installed in the Docker base image used) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "# -------------------------------------\n",
    "#   Convert a Python function into a\n",
    "#     Kubeflow Pipelines Operation \n",
    "# -------------------------------------\n",
    "\n",
    "download_op = comp.func_to_container_op(download_dataset,\n",
    "                                        base_image='tensorflow/tensorflow:1.14.0-py3' )\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Finally, we can also create a **Kubeflow pipeline component**, by packaging the Python Function in a **Docker Image**, as we are going to do in the next steps. Remember that building Docker Image can take severals minutes\n",
    "\n",
    "  Remark: At the time of writing, there is no official Tensorflow 2.0 Docker image available yet, so we manually install Tensorflow 2.0 beta on top of a Tensorflow CUDA 10 image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash \n",
    "\n",
    "# -------------------------------------\n",
    "#     Define a Dockerfile, Build a  \n",
    "#       Docker image to package \n",
    "#   the Kubeflow Pipelines Operation \n",
    "# -------------------------------------\n",
    "\n",
    "cat > ./Dockerfile <<- \"EOF\"\n",
    "FROM tensorflow/tensorflow:1.14.0-py3\n",
    "RUN pip install --quiet tensorflow==2.0.0-beta1\n",
    "ADD ./download_dataset.py /ml/download_dataset.py\n",
    "WORKDIR /ml\n",
    "\n",
    "ENTRYPOINT [\"python\", \"/ml/download_dataset.py\"]\n",
    "\n",
    "EOF\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You need a Docker Registry for storing the Docker image. If you are using GCP/GKE, you can use the **Google Cloud Build** tools to build the image and store them in your Google Storage : Execute the following code **AFTER having set your own GCP PROJECT ID** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------- REMOTE BUILD OUTPUT ------------------------------\n",
      "starting build \"93ad71af-87e6-430d-b3f4-70c5a1f0f65f\"\n",
      "\n",
      "FETCHSOURCE\n",
      "Fetching storage object: gs://wl-tex10-kfp-001_cloudbuild/source/1562948359.35-19e85efaeb72452c8af97a37b9aaa385.tgz#1562948375819672\n",
      "Copying gs://wl-tex10-kfp-001_cloudbuild/source/1562948359.35-19e85efaeb72452c8af97a37b9aaa385.tgz#1562948375819672...\n",
      "/ [0 files][    0.0 B/115.0 MiB]                                                \r",
      "-\r",
      "- [0 files][ 54.1 MiB/115.0 MiB]                                                \r",
      "\\\r",
      "\\ [1 files][115.0 MiB/115.0 MiB]                                                \r",
      "|\r\n",
      "Operation completed over 1 objects/115.0 MiB.                                    \n",
      "BUILD\n",
      "Already have image (with digest): gcr.io/cloud-builders/docker\n",
      "Sending build context to Docker daemon  206.9MB\r",
      "\r\n",
      "Step 1/5 : FROM tensorflow/tensorflow:1.14.0-py3\n",
      "1.14.0-py3: Pulling from tensorflow/tensorflow\n",
      "5b7339215d1d: Already exists\n",
      "14ca88e9f672: Already exists\n",
      "a31c3b1caad4: Already exists\n",
      "b054a26005b7: Already exists\n",
      "8832e3773578: Pulling fs layer\n",
      "5e671b828b2a: Pulling fs layer\n",
      "2b940936f993: Pulling fs layer\n",
      "016724bbd2c9: Pulling fs layer\n",
      "5bd1cb597025: Pulling fs layer\n",
      "68543864d644: Pulling fs layer\n",
      "016724bbd2c9: Waiting\n",
      "5bd1cb597025: Waiting\n",
      "68543864d644: Waiting\n",
      "2b940936f993: Verifying Checksum\n",
      "2b940936f993: Download complete\n",
      "5e671b828b2a: Verifying Checksum\n",
      "5e671b828b2a: Download complete\n",
      "5bd1cb597025: Verifying Checksum\n",
      "5bd1cb597025: Download complete\n",
      "8832e3773578: Verifying Checksum\n",
      "8832e3773578: Download complete\n",
      "68543864d644: Verifying Checksum\n",
      "68543864d644: Download complete\n",
      "016724bbd2c9: Verifying Checksum\n",
      "016724bbd2c9: Download complete\n",
      "8832e3773578: Pull complete\n",
      "5e671b828b2a: Pull complete\n",
      "2b940936f993: Pull complete\n",
      "016724bbd2c9: Pull complete\n",
      "5bd1cb597025: Pull complete\n",
      "68543864d644: Pull complete\n",
      "Digest: sha256:7c05dfab9ea82c95b571ca7dc3d92d4e0b289eeec6b3abf51f189caca0684488\n",
      "Status: Downloaded newer image for tensorflow/tensorflow:1.14.0-py3\n",
      " ---> 4cc892a3babd\n",
      "Step 2/5 : RUN pip install --quiet tensorflow==2.0.0-beta1\n",
      " ---> Running in 91a37bb8d46f\n",
      "Removing intermediate container 91a37bb8d46f\n",
      " ---> 2fa915bd9362\n",
      "Step 3/5 : ADD ./download_dataset.py /ml/download_dataset.py\n",
      " ---> 4d9bdcf38fbd\n",
      "Step 4/5 : WORKDIR /ml\n",
      " ---> Running in 0e5af078a2b2\n",
      "Removing intermediate container 0e5af078a2b2\n",
      " ---> d974bf49a878\n",
      "Step 5/5 : ENTRYPOINT [\"python\", \"/ml/download_dataset.py\"]\n",
      " ---> Running in 51fc0fc82f8e\n",
      "Removing intermediate container 51fc0fc82f8e\n",
      " ---> f47508c808a3\n",
      "Successfully built f47508c808a3\n",
      "Successfully tagged gcr.io/wl-tex10-kfp-001/download_dataset:latest\n",
      "PUSH\n",
      "Pushing gcr.io/wl-tex10-kfp-001/download_dataset:latest\n",
      "The push refers to repository [gcr.io/wl-tex10-kfp-001/download_dataset]\n",
      "8190b9e60ebb: Preparing\n",
      "ee73854277b1: Preparing\n",
      "a144de6e67e6: Preparing\n",
      "4b8ec9124f1c: Preparing\n",
      "652cdcb17d30: Preparing\n",
      "dd7f77c80a16: Preparing\n",
      "f4cb77175ac9: Preparing\n",
      "31835e84bcc0: Preparing\n",
      "75e70aa52609: Preparing\n",
      "dda151859818: Preparing\n",
      "fbd2732ad777: Preparing\n",
      "ba9de9d8475e: Preparing\n",
      "dd7f77c80a16: Waiting\n",
      "f4cb77175ac9: Waiting\n",
      "31835e84bcc0: Waiting\n",
      "75e70aa52609: Waiting\n",
      "dda151859818: Waiting\n",
      "fbd2732ad777: Waiting\n",
      "ba9de9d8475e: Waiting\n",
      "a144de6e67e6: Layer already exists\n",
      "652cdcb17d30: Layer already exists\n",
      "4b8ec9124f1c: Layer already exists\n",
      "dd7f77c80a16: Layer already exists\n",
      "31835e84bcc0: Layer already exists\n",
      "f4cb77175ac9: Layer already exists\n",
      "fbd2732ad777: Layer already exists\n",
      "dda151859818: Layer already exists\n",
      "75e70aa52609: Layer already exists\n",
      "ba9de9d8475e: Layer already exists\n",
      "8190b9e60ebb: Pushed\n",
      "ee73854277b1: Pushed\n",
      "latest: digest: sha256:dbd68b6c4bd27f35de9c4fb6e87dec443578535e0fc17b47831eeb4c19a530a5 size: 2833\n",
      "DONE\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "ID                                    CREATE_TIME                DURATION  SOURCE                                                                                      IMAGES                                              STATUS\n",
      "93ad71af-87e6-430d-b3f4-70c5a1f0f65f  2019-07-12T16:19:36+00:00  2M2S      gs://wl-tex10-kfp-001_cloudbuild/source/1562948359.35-19e85efaeb72452c8af97a37b9aaa385.tgz  gcr.io/wl-tex10-kfp-001/download_dataset (+1 more)  SUCCESS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Activated service account credentials for: [kubeflow-user@wl-tex10-kfp-001.iam.gserviceaccount.com]\n",
      "Creating temporary tarball archive of 367 file(s) totalling 196.9 MiB before compression.\n",
      "Uploading tarball of [.] to [gs://wl-tex10-kfp-001_cloudbuild/source/1562948359.35-19e85efaeb72452c8af97a37b9aaa385.tgz]\n",
      "Created [https://cloudbuild.googleapis.com/v1/projects/wl-tex10-kfp-001/builds/93ad71af-87e6-430d-b3f4-70c5a1f0f65f].\n",
      "Logs are available at [https://console.cloud.google.com/gcr/builds/93ad71af-87e6-430d-b3f4-70c5a1f0f65f?project=612745165979].\n"
     ]
    }
   ],
   "source": [
    "%%bash -s \"$GCP_PROJECT_ID\"\n",
    "\n",
    "# -------------------------------------\n",
    "#       Build a Docker Image on GCP \n",
    "#         using the gcloud tool\n",
    "# -------------------------------------\n",
    "\n",
    "gcloud auth activate-service-account --key-file=${GOOGLE_APPLICATION_CREDENTIALS}\n",
    "gcloud builds submit --tag gcr.io/$1/download_dataset:latest ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now create the other Kubeflow Pipelines Components\n",
    "\n",
    "- Prepare Dataset component: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------- REMOTE BUILD OUTPUT ------------------------------\n",
      "starting build \"c26ac495-d4e7-4e38-b656-a5234977f7d4\"\n",
      "\n",
      "FETCHSOURCE\n",
      "Fetching storage object: gs://wl-tex10-kfp-001_cloudbuild/source/1562948504.11-6e364859b0894ba9bc2598f6f3f85a9f.tgz#1562948520434643\n",
      "Copying gs://wl-tex10-kfp-001_cloudbuild/source/1562948504.11-6e364859b0894ba9bc2598f6f3f85a9f.tgz#1562948520434643...\n",
      "/ [0 files][    0.0 B/115.0 MiB]                                                \r",
      "-\r",
      "- [0 files][ 42.3 MiB/115.0 MiB]                                                \r",
      "\\\r",
      "\\ [1 files][115.0 MiB/115.0 MiB]                                                \r",
      "|\r\n",
      "Operation completed over 1 objects/115.0 MiB.                                    \n",
      "BUILD\n",
      "Already have image (with digest): gcr.io/cloud-builders/docker\n",
      "Sending build context to Docker daemon    207MB\r",
      "\r\n",
      "Step 1/5 : FROM tensorflow/tensorflow:1.14.0-py3\n",
      "1.14.0-py3: Pulling from tensorflow/tensorflow\n",
      "5b7339215d1d: Already exists\n",
      "14ca88e9f672: Already exists\n",
      "a31c3b1caad4: Already exists\n",
      "b054a26005b7: Already exists\n",
      "8832e3773578: Pulling fs layer\n",
      "5e671b828b2a: Pulling fs layer\n",
      "2b940936f993: Pulling fs layer\n",
      "016724bbd2c9: Pulling fs layer\n",
      "5bd1cb597025: Pulling fs layer\n",
      "68543864d644: Pulling fs layer\n",
      "016724bbd2c9: Waiting\n",
      "5bd1cb597025: Waiting\n",
      "68543864d644: Waiting\n",
      "2b940936f993: Verifying Checksum\n",
      "2b940936f993: Download complete\n",
      "5e671b828b2a: Verifying Checksum\n",
      "5e671b828b2a: Download complete\n",
      "5bd1cb597025: Verifying Checksum\n",
      "5bd1cb597025: Download complete\n",
      "8832e3773578: Verifying Checksum\n",
      "8832e3773578: Download complete\n",
      "68543864d644: Verifying Checksum\n",
      "68543864d644: Download complete\n",
      "016724bbd2c9: Verifying Checksum\n",
      "016724bbd2c9: Download complete\n",
      "8832e3773578: Pull complete\n",
      "5e671b828b2a: Pull complete\n",
      "2b940936f993: Pull complete\n",
      "016724bbd2c9: Pull complete\n",
      "5bd1cb597025: Pull complete\n",
      "68543864d644: Pull complete\n",
      "Digest: sha256:7c05dfab9ea82c95b571ca7dc3d92d4e0b289eeec6b3abf51f189caca0684488\n",
      "Status: Downloaded newer image for tensorflow/tensorflow:1.14.0-py3\n",
      " ---> 4cc892a3babd\n",
      "Step 2/5 : RUN pip install --quiet tensorflow==2.0.0-beta1 Pillow\n",
      " ---> Running in 57294c624619\n",
      "Removing intermediate container 57294c624619\n",
      " ---> 4f51b2428218\n",
      "Step 3/5 : ADD ./prepare_dataset.py /ml/prepare_dataset.py\n",
      " ---> b745bd3247fa\n",
      "Step 4/5 : WORKDIR /ml\n",
      " ---> Running in 4cfb506e6965\n",
      "Removing intermediate container 4cfb506e6965\n",
      " ---> b01e3978c2e8\n",
      "Step 5/5 : ENTRYPOINT [\"python\", \"/ml/prepare_dataset.py\"]\n",
      " ---> Running in 479aca28d6bc\n",
      "Removing intermediate container 479aca28d6bc\n",
      " ---> f07bee080122\n",
      "Successfully built f07bee080122\n",
      "Successfully tagged gcr.io/wl-tex10-kfp-001/prepare_dataset:latest\n",
      "PUSH\n",
      "Pushing gcr.io/wl-tex10-kfp-001/prepare_dataset:latest\n",
      "The push refers to repository [gcr.io/wl-tex10-kfp-001/prepare_dataset]\n",
      "e6dc2fa14d0a: Preparing\n",
      "d3535fc1afef: Preparing\n",
      "a144de6e67e6: Preparing\n",
      "4b8ec9124f1c: Preparing\n",
      "652cdcb17d30: Preparing\n",
      "dd7f77c80a16: Preparing\n",
      "f4cb77175ac9: Preparing\n",
      "31835e84bcc0: Preparing\n",
      "75e70aa52609: Preparing\n",
      "dda151859818: Preparing\n",
      "fbd2732ad777: Preparing\n",
      "ba9de9d8475e: Preparing\n",
      "dd7f77c80a16: Waiting\n",
      "f4cb77175ac9: Waiting\n",
      "31835e84bcc0: Waiting\n",
      "75e70aa52609: Waiting\n",
      "dda151859818: Waiting\n",
      "fbd2732ad777: Waiting\n",
      "ba9de9d8475e: Waiting\n",
      "4b8ec9124f1c: Layer already exists\n",
      "652cdcb17d30: Layer already exists\n",
      "a144de6e67e6: Layer already exists\n",
      "f4cb77175ac9: Layer already exists\n",
      "dd7f77c80a16: Layer already exists\n",
      "75e70aa52609: Layer already exists\n",
      "31835e84bcc0: Layer already exists\n",
      "dda151859818: Layer already exists\n",
      "fbd2732ad777: Layer already exists\n",
      "ba9de9d8475e: Layer already exists\n",
      "e6dc2fa14d0a: Pushed\n",
      "d3535fc1afef: Pushed\n",
      "latest: digest: sha256:a76f65b536b7f7d1e31e2ac8ac8f3ab2de6fbe943fb2fe440a299d6982db61d9 size: 2833\n",
      "DONE\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "ID                                    CREATE_TIME                DURATION  SOURCE                                                                                      IMAGES                                             STATUS\n",
      "c26ac495-d4e7-4e38-b656-a5234977f7d4  2019-07-12T16:22:01+00:00  2M1S      gs://wl-tex10-kfp-001_cloudbuild/source/1562948504.11-6e364859b0894ba9bc2598f6f3f85a9f.tgz  gcr.io/wl-tex10-kfp-001/prepare_dataset (+1 more)  SUCCESS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Activated service account credentials for: [kubeflow-user@wl-tex10-kfp-001.iam.gserviceaccount.com]\n",
      "Creating temporary tarball archive of 369 file(s) totalling 197.0 MiB before compression.\n",
      "Uploading tarball of [.] to [gs://wl-tex10-kfp-001_cloudbuild/source/1562948504.11-6e364859b0894ba9bc2598f6f3f85a9f.tgz]\n",
      "Created [https://cloudbuild.googleapis.com/v1/projects/wl-tex10-kfp-001/builds/c26ac495-d4e7-4e38-b656-a5234977f7d4].\n",
      "Logs are available at [https://console.cloud.google.com/gcr/builds/c26ac495-d4e7-4e38-b656-a5234977f7d4?project=612745165979].\n"
     ]
    }
   ],
   "source": [
    "%%bash -s \"$GCP_PROJECT_ID\"\n",
    "\n",
    "# -------------------------------------\n",
    "#     Define a Dockerfile, Build a  \n",
    "#       Docker image to package \n",
    "#   the Kubeflow Pipelines Operation \n",
    "# -------------------------------------\n",
    "\n",
    "cat > ./Dockerfile <<- \"EOF\"\n",
    "FROM tensorflow/tensorflow:1.14.0-py3\n",
    "RUN pip install --quiet tensorflow==2.0.0-beta1 Pillow \n",
    "ADD ./prepare_dataset.py /ml/prepare_dataset.py\n",
    "WORKDIR /ml\n",
    "\n",
    "ENTRYPOINT [\"python\", \"/ml/prepare_dataset.py\"]\n",
    "\n",
    "EOF\n",
    "\n",
    "# -------------------------------------\n",
    "#       Build a Docker Image on GCP \n",
    "#         using the gcloud tool\n",
    "# -------------------------------------\n",
    "\n",
    "gcloud auth activate-service-account --key-file=${GOOGLE_APPLICATION_CREDENTIALS}\n",
    "gcloud builds submit --tag gcr.io/$1/prepare_dataset:latest .\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**WIP Investigate : kfp.compiler.build_docker_image ?**\n",
    "\n",
    "!gsutil cp ./download_dataset.py gs://wl-tex10-kfp-001/docker/download_dataset.py\n",
    "!gsutil cp ./Dockerfile gs://wl-tex10-kfp-001/docker/Dockerfile\n",
    "\n",
    "\n",
    "kfp.compiler.build_docker_image(staging_gcs_path='gs://wl-tex10-kfp-001/staging', \n",
    "                                target_image='gcr.io/{}/test:latest'.format(GCP_PROJECT_ID), \n",
    "                                dockerfile_path='gs://wl-tex10-kfp-001/docker/Dockerfile', \n",
    "                                timeout=600, namespace='kubeflow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### WIP : TEST A SINGLE COMPONENT PIPELINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------- \n",
    "# Create Kubeflow Pipelines Operations\n",
    "#        from Docker Images\n",
    "# -------------------------------------\n",
    "\n",
    "def download_op(fname=FILE_NAME, origin=URL, \n",
    "                cachedir=KERAS_CACHE_DIR, cachesubdir='datasets'):\n",
    "    \n",
    "    return kfp.dsl.ContainerOp(name=\"Download Dataset\",\n",
    "                               image='gcr.io/{}/download_dataset:latest'.format(GCP_PROJECT_ID),\n",
    "                               command=['python', '/ml/download_dataset.py'],\n",
    "                               arguments=['--fname', fname,\n",
    "                                          '--origin', origin,\n",
    "                                          '--cachedir', cachedir,\n",
    "                                          '--cachesubdir', cachesubdir ],\n",
    "                               file_outputs = {'outputdir': '/output.txt'}\n",
    "                              )\n",
    "\n",
    "\n",
    "def prepare_dataset_op(pathimg, pathimgsubdir, op_name):\n",
    "\n",
    "    return kfp.dsl.ContainerOp(name=op_name,\n",
    "                               image='gcr.io/{}/prepare_dataset:latest'.format(GCP_PROJECT_ID),\n",
    "                               command=['python', '/ml/prepare_dataset.py'],\n",
    "                               arguments=['--pathimg', pathimg,\n",
    "                                          '--pathimgsubdir', pathimgsubdir ],\n",
    "                               file_outputs = {'outputdir': '/output.txt'}\n",
    "                              )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------- \n",
    "#  Build the pix2pix Pipeline Function\n",
    "# -------------------------------------\n",
    "\n",
    "@dsl.pipeline(\n",
    "    name='TEST pipeline',\n",
    "    description='A pipeline to download and prepare the dataset'\n",
    ")\n",
    "\n",
    "def pix2pix(\n",
    "    \n",
    "    ## -- Download Dataset Kubeflow Pipeline component parameters (with default values)\n",
    "    origin = dsl.PipelineParam('origin', value=URL),\n",
    "    fname = dsl.PipelineParam('fname', value=FILE_NAME),\n",
    "    cachedir = dsl.PipelineParam('cachedir', value=KERAS_CACHE_DIR), # on Kubeflow GKE/NFS \n",
    "    cachesubdir = dsl.PipelineParam('cachesubdir', value=\"datasets\"),\n",
    "    \n",
    "    ## -- Prepare Dataset Kubeflow Pipeline component parameters (with default values)\n",
    "    pathimgsubdirtrain = dsl.PipelineParam('pathimgsubdirtrain', value=\"train\"),\n",
    "    pathimgsubdirtest = dsl.PipelineParam('pathimgsubdirtest', value=\"test\"),\n",
    "       \n",
    "):\n",
    "    \n",
    "    # Passing pipeline parameters as operation arguments (Returns a dsl.ContainerOp class instance)\n",
    "    download_task = download_op(fname, origin, cachedir, cachesubdir) \\\n",
    "                                .add_volume(k8s_client.V1Volume(name='workdir', \n",
    "                                                                persistent_volume_claim=k8s_client.V1PersistentVolumeClaimVolumeSource(claim_name='nfs'))) \\\n",
    "                                .add_volume_mount(k8s_client.V1VolumeMount(mount_path=NFS_MOUNT, name='workdir'))\n",
    "    \n",
    "    \n",
    "    prepare_train_task = prepare_dataset_op(pathimg=download_task.output, \n",
    "                                            pathimgsubdir=pathimgsubdirtrain,\n",
    "                                            op_name=\"Prepare Train Dataset\") \\\n",
    "                                .add_volume(k8s_client.V1Volume(name='workdir', \n",
    "                                                                persistent_volume_claim=k8s_client.V1PersistentVolumeClaimVolumeSource(claim_name='nfs'))) \\\n",
    "                                .add_volume_mount(k8s_client.V1VolumeMount(mount_path=NFS_MOUNT, name='workdir'))\n",
    "        \n",
    "\n",
    "    prepare_test_task = prepare_dataset_op(pathimg=download_task.output,\n",
    "                                           pathimgsubdir=pathimgsubdirtest,\n",
    "                                           op_name=\"Prepare test Dataset\") \\\n",
    "                                .add_volume(k8s_client.V1Volume(name='workdir', \n",
    "                                                                persistent_volume_claim=k8s_client.V1PersistentVolumeClaimVolumeSource(claim_name='nfs'))) \\\n",
    "                                .add_volume_mount(k8s_client.V1VolumeMount(mount_path=NFS_MOUNT, name='workdir'))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------\n",
    "#       Compile the Pipeline \n",
    "# -------------------------------------\n",
    "\n",
    "pipeline_filename = pix2pix.__name__ + '.pipeline.tar.gz'\n",
    "compiler.Compiler().compile(pipeline_func=pix2pix, \n",
    "                            package_path=pipeline_filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Run link <a href=\"/pipeline/#/runs/details/78066178-a4c3-11e9-b468-42010a8400ef\" target=\"_blank\" >here</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ------------------------------------- \n",
    "#           Create (or reuse) a    \n",
    "#      Kubeflow Pipeline Experiment\n",
    "# -------------------------------------\n",
    "\n",
    "EXPERIMENT_NAME = \"TEST - Pix2Pix\"   ## Customize Name\n",
    "client = kfp.Client()\n",
    "\n",
    "try:\n",
    "    experiment = client.get_experiment(experiment_name=EXPERIMENT_NAME)\n",
    "except:\n",
    "    experiment = client.create_experiment(EXPERIMENT_NAME)\n",
    "\n",
    "# -------------------------------------\n",
    "#              Optional : \n",
    "# Specify/Overwrite pipeline arguments \n",
    "#     default values for execution\n",
    "# -------------------------------------\n",
    "\n",
    "#arguments = {'epochs': 1 }# Change to 200 for a full training \n",
    "\n",
    "# -------------------------------------\n",
    "#       Submit a pipeline run\n",
    "# -------------------------------------\n",
    "run_name = pix2pix.__name__ + ' run'\n",
    "run_result = client.run_pipeline(experiment.id, run_name, pipeline_filename)\n",
    "#run_result = client.run_pipeline(experiment.id, run_name, pipeline_filename, arguments)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls /mnt/nfs/data/datasets/facades"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "rm -rf /mnt/nfs/data/*\n",
    "rm -f ./Dockerfile\n",
    "rm -f *.tar.gz"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "last_runtime": {
    "build_target": "//learning/brain/python/client:colab_notebook",
    "kind": "private"
   },
   "name": "pix2pix.ipynb",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
