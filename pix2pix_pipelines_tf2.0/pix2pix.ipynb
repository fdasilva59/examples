{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pix2Pix with Tensorflow 2.0 and Kubeflow Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**WIP/TODO**\n",
    "\n",
    "- Add link + copyright to Pix2Pix homepage + Tensorflow 2.0 Tutorials\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Requirements\n",
    "\n",
    "**WIP/TODO**\n",
    "\n",
    "- Setup a Kubeflow Cluster\n",
    "    - Easiest : Deploy Kubeflow on GKE : Cloud Deploy \n",
    "    - Customize installation (Add NFS Storage, Google Storage, GPU node pool, \"gcloud services enable ')\n",
    "       - GPU require a Google Account with Billing Enabled\n",
    "       - Get 300$ of FREE credits\n",
    "    - Start a Jupyter Server on Kubeflow\n",
    "    - Clone this project repository in your Jupyter Notebook environment\n",
    "\n",
    "\n",
    "- Learn more on Kubeflow...\n",
    "- Learn more on Tensorflow 2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What you will learn\n",
    "\n",
    "**WIP/TODO** \n",
    "- Use Kubelow to create a Deep Learning Pipeline for Tensorflow 2.0 (using GPU)\n",
    "- Learn to create Kubeflow Pipelines Components\n",
    "- Define and execute a Kubelow pipelines from this Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Word on Pix2Pix and the Training Pipeline we will define below\n",
    "\n",
    "**WIP/TODO**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get ready for the execution of this Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------\n",
    "#           IMPORTANT : \n",
    "#      Customize this variable with \n",
    "#        your own GCP Project ID\n",
    "#  (will be used by Google Cloud Build)\n",
    "# -------------------------------------\n",
    "\n",
    "\n",
    "GCP_PROJECT_ID=None\n",
    "\n",
    "assert (GCP_PROJECT_ID) != None, \"Your must set your own GCP Project ID\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------\n",
    "#    Notebook configuration 'magic'\n",
    "# -------------------------------------\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%config InlineBackend.figure_format = 'retina'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If necessary install the latest Kubeflow Pipelines SDK. This Python library allows to define a pipeline composed of multiple tasks, and execute it on a Kubeflow Cluster "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# -------------------------------------\n",
    "#   Install Kubeflow Pipelines SDK\n",
    "# -------------------------------------\n",
    "\n",
    "!pip3 install https://storage.googleapis.com/ml-pipeline/release/0.1.20/kfp.tar.gz \\\n",
    "              --quiet --disable-pip-version-check --upgrade\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------\n",
    "#     Import Kubeflow Pipelines SDK \n",
    "# -------------------------------------\n",
    "\n",
    "import kfp\n",
    "import kfp.dsl as dsl\n",
    "import kfp.gcp as gcp\n",
    "import kfp.notebook\n",
    "import kfp.components as comp\n",
    "from kfp import compiler\n",
    "from kubernetes import client as k8s_client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------\n",
    "#          Import PiX2Pix code \n",
    "# -------------------------------------\n",
    "\n",
    "from download_dataset import *\n",
    "from prepare_dataset import *\n",
    "from train_pix2pix import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------\n",
    "#    DEFINE SOME PROJECT VARIABLES\n",
    "# -------------------------------------\n",
    "URL = \"https://people.eecs.berkeley.edu/~tinghuiz/projects/pix2pix/datasets/facades.tar.gz\"\n",
    "FILE_NAME = \"facades.tar.gz\"\n",
    "\n",
    "NFS_MOUNT = \"/mnt/nfs\"\n",
    "KERAS_CACHE_DIR = \"/mnt/nfs/data/\"\n",
    "\n",
    "MODEL = \"/mnt/nfs/data/models/\"\n",
    "PIX2PIX_OUTPUTS = \"/mnt/nfs/data/pix2pix_outputs/\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learn to Create Kubeflow Pipelines Components "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 3 ways of using the Pix2Pix source code:\n",
    "\n",
    "-  First, **without using Kubeflow**, we can normaly use the python code from download_dataset.py as for any other python code. For example:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "````\n",
    "# -------------------------------------\n",
    "#       Local Python execution \n",
    "#           (No Kubeflow)\n",
    "# -------------------------------------\n",
    "\n",
    "!python download_dataset.py --fname facades.tar.gz \\ \n",
    "--origin \"https://people.eecs.berkeley.edu/~tinghuiz/projects/pix2pix/datasets/facades.tar.gz\"\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can also create a **Kubeflow pipeline component**, by simply converting a Python function into a Kubeflow Pipelines Operation, as in the code example below:\n",
    "\n",
    "   (**However, make sure** that all the python libraries that will be used are installed in the Docker base image used) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "# -------------------------------------\n",
    "#   Convert a Python function into a\n",
    "#     Kubeflow Pipelines Operation \n",
    "# -------------------------------------\n",
    "\n",
    "download_op = comp.func_to_container_op(download_dataset,\n",
    "                                        base_image='tensorflow/tensorflow:1.14.0-py3' )\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Finally, we can also create a **Kubeflow pipeline component**, by packaging the Python Function in a **Docker Image**, as we are going to do in the next steps. Remember that building Docker Image can take severals minutes\n",
    "\n",
    "  Remark: At the time of writing, there is no official Tensorflow 2.0 Docker image available yet, so we manually install Tensorflow 2.0 beta on top of a Tensorflow CUDA 10 image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash \n",
    "\n",
    "# -------------------------------------\n",
    "#     Define a Dockerfile, Build a  \n",
    "#       Docker image to package \n",
    "#   the Kubeflow Pipelines Operation \n",
    "# -------------------------------------\n",
    "\n",
    "cat > ./Dockerfile <<- \"EOF\"\n",
    "FROM tensorflow/tensorflow:1.14.0-py3\n",
    "RUN pip install --quiet tensorflow==2.0.0-beta1\n",
    "ADD ./download_dataset.py /ml/download_dataset.py\n",
    "WORKDIR /ml\n",
    "\n",
    "ENTRYPOINT [\"python\", \"/ml/download_dataset.py\"]\n",
    "\n",
    "EOF\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You need a Docker Registry for storing the Docker image. If you are using GCP/GKE, you can use the **Google Cloud Build** tools to build the image and store them in your Google Storage : Execute the following code **AFTER having set your own GCP PROJECT ID** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------- REMOTE BUILD OUTPUT ------------------------------\n",
      "starting build \"4b91137b-fee9-445e-bf5f-e3831947bf68\"\n",
      "\n",
      "FETCHSOURCE\n",
      "Fetching storage object: gs://wl-tex10-kfp-001_cloudbuild/source/1562960926.18-17a587f6d097441cb3861cd2fe104355.tgz#1562960950097965\n",
      "Copying gs://wl-tex10-kfp-001_cloudbuild/source/1562960926.18-17a587f6d097441cb3861cd2fe104355.tgz#1562960950097965...\n",
      "/ [0 files][    0.0 B/178.2 MiB]                                                \r",
      "-\r",
      "- [0 files][ 40.5 MiB/178.2 MiB]                                                \r",
      "\\\r",
      "|\r",
      "| [0 files][145.7 MiB/178.2 MiB]                                                \r",
      "| [1 files][178.2 MiB/178.2 MiB]                                                \r\n",
      "Operation completed over 1 objects/178.2 MiB.                                    \n",
      "BUILD\n",
      "Already have image (with digest): gcr.io/cloud-builders/docker\n",
      "Sending build context to Docker daemon  308.1MB\r",
      "\r\n",
      "Step 1/5 : FROM tensorflow/tensorflow:1.14.0-py3\n",
      "1.14.0-py3: Pulling from tensorflow/tensorflow\n",
      "5b7339215d1d: Already exists\n",
      "14ca88e9f672: Already exists\n",
      "a31c3b1caad4: Already exists\n",
      "b054a26005b7: Already exists\n",
      "8832e3773578: Pulling fs layer\n",
      "5e671b828b2a: Pulling fs layer\n",
      "2b940936f993: Pulling fs layer\n",
      "016724bbd2c9: Pulling fs layer\n",
      "5bd1cb597025: Pulling fs layer\n",
      "68543864d644: Pulling fs layer\n",
      "016724bbd2c9: Waiting\n",
      "5bd1cb597025: Waiting\n",
      "68543864d644: Waiting\n",
      "2b940936f993: Verifying Checksum\n",
      "2b940936f993: Download complete\n",
      "5e671b828b2a: Verifying Checksum\n",
      "5e671b828b2a: Download complete\n",
      "5bd1cb597025: Verifying Checksum\n",
      "5bd1cb597025: Download complete\n",
      "8832e3773578: Verifying Checksum\n",
      "8832e3773578: Download complete\n",
      "68543864d644: Verifying Checksum\n",
      "68543864d644: Download complete\n",
      "016724bbd2c9: Verifying Checksum\n",
      "016724bbd2c9: Download complete\n",
      "8832e3773578: Pull complete\n",
      "5e671b828b2a: Pull complete\n",
      "2b940936f993: Pull complete\n",
      "016724bbd2c9: Pull complete\n",
      "5bd1cb597025: Pull complete\n",
      "68543864d644: Pull complete\n",
      "Digest: sha256:7c05dfab9ea82c95b571ca7dc3d92d4e0b289eeec6b3abf51f189caca0684488\n",
      "Status: Downloaded newer image for tensorflow/tensorflow:1.14.0-py3\n",
      " ---> 4cc892a3babd\n",
      "Step 2/5 : RUN pip install --quiet tensorflow==2.0.0-beta1\n",
      " ---> Running in 434ba8f34933\n",
      "Removing intermediate container 434ba8f34933\n",
      " ---> 9060dec7fb9c\n",
      "Step 3/5 : ADD ./download_dataset.py /ml/download_dataset.py\n",
      " ---> 1324510dbaf2\n",
      "Step 4/5 : WORKDIR /ml\n",
      " ---> Running in 1c9b897b3540\n",
      "Removing intermediate container 1c9b897b3540\n",
      " ---> cd20d739e026\n",
      "Step 5/5 : ENTRYPOINT [\"python\", \"/ml/download_dataset.py\"]\n",
      " ---> Running in 12ac6bd917c8\n",
      "Removing intermediate container 12ac6bd917c8\n",
      " ---> dc0945dff970\n",
      "Successfully built dc0945dff970\n",
      "Successfully tagged gcr.io/wl-tex10-kfp-001/download_dataset:latest\n",
      "PUSH\n",
      "Pushing gcr.io/wl-tex10-kfp-001/download_dataset:latest\n",
      "The push refers to repository [gcr.io/wl-tex10-kfp-001/download_dataset]\n",
      "78df159cdd04: Preparing\n",
      "d0da9131bb8b: Preparing\n",
      "a144de6e67e6: Preparing\n",
      "4b8ec9124f1c: Preparing\n",
      "652cdcb17d30: Preparing\n",
      "dd7f77c80a16: Preparing\n",
      "f4cb77175ac9: Preparing\n",
      "31835e84bcc0: Preparing\n",
      "75e70aa52609: Preparing\n",
      "dda151859818: Preparing\n",
      "fbd2732ad777: Preparing\n",
      "ba9de9d8475e: Preparing\n",
      "dd7f77c80a16: Waiting\n",
      "f4cb77175ac9: Waiting\n",
      "31835e84bcc0: Waiting\n",
      "75e70aa52609: Waiting\n",
      "dda151859818: Waiting\n",
      "fbd2732ad777: Waiting\n",
      "ba9de9d8475e: Waiting\n",
      "4b8ec9124f1c: Layer already exists\n",
      "a144de6e67e6: Layer already exists\n",
      "652cdcb17d30: Layer already exists\n",
      "f4cb77175ac9: Layer already exists\n",
      "dd7f77c80a16: Layer already exists\n",
      "31835e84bcc0: Layer already exists\n",
      "dda151859818: Layer already exists\n",
      "75e70aa52609: Layer already exists\n",
      "fbd2732ad777: Layer already exists\n",
      "ba9de9d8475e: Layer already exists\n",
      "78df159cdd04: Pushed\n",
      "d0da9131bb8b: Pushed\n",
      "latest: digest: sha256:bf921c8e1fc6155b6d9d41887cf87aceac97664d05d183a93dba11e6d5ba48e4 size: 2833\n",
      "DONE\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "ID                                    CREATE_TIME                DURATION  SOURCE                                                                                      IMAGES                                              STATUS\n",
      "4b91137b-fee9-445e-bf5f-e3831947bf68  2019-07-12T19:49:10+00:00  2M1S      gs://wl-tex10-kfp-001_cloudbuild/source/1562960926.18-17a587f6d097441cb3861cd2fe104355.tgz  gcr.io/wl-tex10-kfp-001/download_dataset (+1 more)  SUCCESS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Activated service account credentials for: [kubeflow-user@wl-tex10-kfp-001.iam.gserviceaccount.com]\n",
      "Creating temporary tarball archive of 1500 file(s) totalling 292.2 MiB before compression.\n",
      "Uploading tarball of [.] to [gs://wl-tex10-kfp-001_cloudbuild/source/1562960926.18-17a587f6d097441cb3861cd2fe104355.tgz]\n",
      "Created [https://cloudbuild.googleapis.com/v1/projects/wl-tex10-kfp-001/builds/4b91137b-fee9-445e-bf5f-e3831947bf68].\n",
      "Logs are available at [https://console.cloud.google.com/gcr/builds/4b91137b-fee9-445e-bf5f-e3831947bf68?project=612745165979].\n"
     ]
    }
   ],
   "source": [
    "%%bash -s \"$GCP_PROJECT_ID\"\n",
    "\n",
    "# -------------------------------------\n",
    "#       Build a Docker Image on GCP \n",
    "#         using the gcloud tool\n",
    "# -------------------------------------\n",
    "\n",
    "gcloud auth activate-service-account --key-file=${GOOGLE_APPLICATION_CREDENTIALS}\n",
    "gcloud builds submit --tag gcr.io/$1/download_dataset:latest ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now create the other Kubeflow Pipelines Components\n",
    "\n",
    "- Prepare Dataset component: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------- REMOTE BUILD OUTPUT ------------------------------\n",
      "starting build \"e32a62b1-cab5-4224-b793-d74441cd3d79\"\n",
      "\n",
      "FETCHSOURCE\n",
      "Fetching storage object: gs://wl-tex10-kfp-001_cloudbuild/source/1562961075.23-7235fb39ffea4ce59ed4730a711e5aa0.tgz#1562961098513370\n",
      "Copying gs://wl-tex10-kfp-001_cloudbuild/source/1562961075.23-7235fb39ffea4ce59ed4730a711e5aa0.tgz#1562961098513370...\n",
      "/ [0 files][    0.0 B/178.3 MiB]                                                \r",
      "-\r",
      "- [0 files][ 18.3 MiB/178.3 MiB]                                                \r",
      "\\\r",
      "|\r",
      "| [0 files][109.8 MiB/178.3 MiB]                                                \r",
      "/\r",
      "/ [1 files][178.3 MiB/178.3 MiB]                                                \r\n",
      "Operation completed over 1 objects/178.3 MiB.                                    \n",
      "BUILD\n",
      "Already have image (with digest): gcr.io/cloud-builders/docker\n",
      "Sending build context to Docker daemon  308.3MB\r",
      "\r\n",
      "Step 1/5 : FROM tensorflow/tensorflow:1.14.0-py3\n",
      "1.14.0-py3: Pulling from tensorflow/tensorflow\n",
      "5b7339215d1d: Already exists\n",
      "14ca88e9f672: Already exists\n",
      "a31c3b1caad4: Already exists\n",
      "b054a26005b7: Already exists\n",
      "8832e3773578: Pulling fs layer\n",
      "5e671b828b2a: Pulling fs layer\n",
      "2b940936f993: Pulling fs layer\n",
      "016724bbd2c9: Pulling fs layer\n",
      "5bd1cb597025: Pulling fs layer\n",
      "68543864d644: Pulling fs layer\n",
      "5bd1cb597025: Waiting\n",
      "68543864d644: Waiting\n",
      "016724bbd2c9: Waiting\n",
      "2b940936f993: Verifying Checksum\n",
      "2b940936f993: Download complete\n",
      "5e671b828b2a: Verifying Checksum\n",
      "5e671b828b2a: Download complete\n",
      "5bd1cb597025: Verifying Checksum\n",
      "5bd1cb597025: Download complete\n",
      "8832e3773578: Verifying Checksum\n",
      "8832e3773578: Download complete\n",
      "68543864d644: Verifying Checksum\n",
      "68543864d644: Download complete\n",
      "016724bbd2c9: Verifying Checksum\n",
      "016724bbd2c9: Download complete\n",
      "8832e3773578: Pull complete\n",
      "5e671b828b2a: Pull complete\n",
      "2b940936f993: Pull complete\n",
      "016724bbd2c9: Pull complete\n",
      "5bd1cb597025: Pull complete\n",
      "68543864d644: Pull complete\n",
      "Digest: sha256:7c05dfab9ea82c95b571ca7dc3d92d4e0b289eeec6b3abf51f189caca0684488\n",
      "Status: Downloaded newer image for tensorflow/tensorflow:1.14.0-py3\n",
      " ---> 4cc892a3babd\n",
      "Step 2/5 : RUN pip install --quiet tensorflow==2.0.0-beta1 Pillow\n",
      " ---> Running in b409437c94a5\n",
      "Removing intermediate container b409437c94a5\n",
      " ---> a573aaec5510\n",
      "Step 3/5 : ADD ./prepare_dataset.py /ml/prepare_dataset.py\n",
      " ---> d56139dfaffd\n",
      "Step 4/5 : WORKDIR /ml\n",
      " ---> Running in 32bcdd4c90c2\n",
      "Removing intermediate container 32bcdd4c90c2\n",
      " ---> 8c46973f58f2\n",
      "Step 5/5 : ENTRYPOINT [\"python\", \"/ml/prepare_dataset.py\"]\n",
      " ---> Running in f30bc007c3a2\n",
      "Removing intermediate container f30bc007c3a2\n",
      " ---> 876bc950024b\n",
      "Successfully built 876bc950024b\n",
      "Successfully tagged gcr.io/wl-tex10-kfp-001/prepare_dataset:latest\n",
      "PUSH\n",
      "Pushing gcr.io/wl-tex10-kfp-001/prepare_dataset:latest\n",
      "The push refers to repository [gcr.io/wl-tex10-kfp-001/prepare_dataset]\n",
      "3afa87ec4da6: Preparing\n",
      "a030d4267951: Preparing\n",
      "a144de6e67e6: Preparing\n",
      "4b8ec9124f1c: Preparing\n",
      "652cdcb17d30: Preparing\n",
      "dd7f77c80a16: Preparing\n",
      "f4cb77175ac9: Preparing\n",
      "31835e84bcc0: Preparing\n",
      "75e70aa52609: Preparing\n",
      "dda151859818: Preparing\n",
      "fbd2732ad777: Preparing\n",
      "ba9de9d8475e: Preparing\n",
      "dd7f77c80a16: Waiting\n",
      "f4cb77175ac9: Waiting\n",
      "31835e84bcc0: Waiting\n",
      "75e70aa52609: Waiting\n",
      "dda151859818: Waiting\n",
      "fbd2732ad777: Waiting\n",
      "ba9de9d8475e: Waiting\n",
      "a144de6e67e6: Layer already exists\n",
      "4b8ec9124f1c: Layer already exists\n",
      "652cdcb17d30: Layer already exists\n",
      "31835e84bcc0: Layer already exists\n",
      "dd7f77c80a16: Layer already exists\n",
      "f4cb77175ac9: Layer already exists\n",
      "fbd2732ad777: Layer already exists\n",
      "dda151859818: Layer already exists\n",
      "75e70aa52609: Layer already exists\n",
      "ba9de9d8475e: Layer already exists\n",
      "3afa87ec4da6: Pushed\n",
      "a030d4267951: Pushed\n",
      "latest: digest: sha256:8b9298234fcf8dd0274795e51add7126b35094f2066369d3c40b4e76d3ce7d00 size: 2833\n",
      "DONE\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "ID                                    CREATE_TIME                DURATION  SOURCE                                                                                      IMAGES                                             STATUS\n",
      "e32a62b1-cab5-4224-b793-d74441cd3d79  2019-07-12T19:51:38+00:00  2M6S      gs://wl-tex10-kfp-001_cloudbuild/source/1562961075.23-7235fb39ffea4ce59ed4730a711e5aa0.tgz  gcr.io/wl-tex10-kfp-001/prepare_dataset (+1 more)  SUCCESS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Activated service account credentials for: [kubeflow-user@wl-tex10-kfp-001.iam.gserviceaccount.com]\n",
      "Creating temporary tarball archive of 1502 file(s) totalling 292.4 MiB before compression.\n",
      "Uploading tarball of [.] to [gs://wl-tex10-kfp-001_cloudbuild/source/1562961075.23-7235fb39ffea4ce59ed4730a711e5aa0.tgz]\n",
      "Created [https://cloudbuild.googleapis.com/v1/projects/wl-tex10-kfp-001/builds/e32a62b1-cab5-4224-b793-d74441cd3d79].\n",
      "Logs are available at [https://console.cloud.google.com/gcr/builds/e32a62b1-cab5-4224-b793-d74441cd3d79?project=612745165979].\n"
     ]
    }
   ],
   "source": [
    "%%bash -s \"$GCP_PROJECT_ID\"\n",
    "\n",
    "# -------------------------------------\n",
    "#     Define a Dockerfile, Build a  \n",
    "#       Docker image to package \n",
    "#   the Kubeflow Pipelines Operation \n",
    "# -------------------------------------\n",
    "\n",
    "cat > ./Dockerfile <<- \"EOF\"\n",
    "FROM tensorflow/tensorflow:1.14.0-py3\n",
    "RUN pip install --quiet tensorflow==2.0.0-beta1 Pillow \n",
    "ADD ./prepare_dataset.py /ml/prepare_dataset.py\n",
    "WORKDIR /ml\n",
    "\n",
    "ENTRYPOINT [\"python\", \"/ml/prepare_dataset.py\"]\n",
    "\n",
    "EOF\n",
    "\n",
    "# -------------------------------------\n",
    "#       Build a Docker Image on GCP \n",
    "#         using the gcloud tool\n",
    "# -------------------------------------\n",
    "\n",
    "gcloud auth activate-service-account --key-file=${GOOGLE_APPLICATION_CREDENTIALS}\n",
    "gcloud builds submit --tag gcr.io/$1/prepare_dataset:latest .\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Train the Pix2Pix model:\n",
    "\n",
    "**WIP** ADD GPU !\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------- REMOTE BUILD OUTPUT ------------------------------\n",
      "starting build \"adc07b1a-85bd-419c-949a-1d5df624692e\"\n",
      "\n",
      "FETCHSOURCE\n",
      "Fetching storage object: gs://wl-tex10-kfp-001_cloudbuild/source/1562961228.22-d216aed74d1d4882a5cead76f238102a.tgz#1562961251813188\n",
      "Copying gs://wl-tex10-kfp-001_cloudbuild/source/1562961228.22-d216aed74d1d4882a5cead76f238102a.tgz#1562961251813188...\n",
      "/ [0 files][    0.0 B/178.3 MiB]                                                \r",
      "-\r",
      "- [0 files][ 25.8 MiB/178.3 MiB]                                                \r",
      "\\\r",
      "|\r",
      "| [0 files][123.0 MiB/178.3 MiB]                                                \r",
      "/\r",
      "/ [1 files][178.3 MiB/178.3 MiB]                                                \r\n",
      "Operation completed over 1 objects/178.3 MiB.                                    \n",
      "BUILD\n",
      "Already have image (with digest): gcr.io/cloud-builders/docker\n",
      "Sending build context to Docker daemon  308.5MB\r",
      "\r\n",
      "Step 1/5 : FROM tensorflow/tensorflow:1.14.0-py3\n",
      "1.14.0-py3: Pulling from tensorflow/tensorflow\n",
      "5b7339215d1d: Already exists\n",
      "14ca88e9f672: Already exists\n",
      "a31c3b1caad4: Already exists\n",
      "b054a26005b7: Already exists\n",
      "8832e3773578: Pulling fs layer\n",
      "5e671b828b2a: Pulling fs layer\n",
      "2b940936f993: Pulling fs layer\n",
      "016724bbd2c9: Pulling fs layer\n",
      "5bd1cb597025: Pulling fs layer\n",
      "68543864d644: Pulling fs layer\n",
      "016724bbd2c9: Waiting\n",
      "5bd1cb597025: Waiting\n",
      "68543864d644: Waiting\n",
      "2b940936f993: Verifying Checksum\n",
      "2b940936f993: Download complete\n",
      "5e671b828b2a: Verifying Checksum\n",
      "5e671b828b2a: Download complete\n",
      "5bd1cb597025: Verifying Checksum\n",
      "5bd1cb597025: Download complete\n",
      "8832e3773578: Verifying Checksum\n",
      "8832e3773578: Download complete\n",
      "68543864d644: Verifying Checksum\n",
      "68543864d644: Download complete\n",
      "016724bbd2c9: Verifying Checksum\n",
      "016724bbd2c9: Download complete\n",
      "8832e3773578: Pull complete\n",
      "5e671b828b2a: Pull complete\n",
      "2b940936f993: Pull complete\n",
      "016724bbd2c9: Pull complete\n",
      "5bd1cb597025: Pull complete\n",
      "68543864d644: Pull complete\n",
      "Digest: sha256:7c05dfab9ea82c95b571ca7dc3d92d4e0b289eeec6b3abf51f189caca0684488\n",
      "Status: Downloaded newer image for tensorflow/tensorflow:1.14.0-py3\n",
      " ---> 4cc892a3babd\n",
      "Step 2/5 : RUN pip install --quiet tensorflow==2.0.0-beta1 Pillow\n",
      " ---> Running in 680b50cef4fa\n",
      "Removing intermediate container 680b50cef4fa\n",
      " ---> 2dea97fa9d6c\n",
      "Step 3/5 : ADD ./train_pix2pix.py /ml/train_pix2pix.py\n",
      " ---> 142f9d352b95\n",
      "Step 4/5 : WORKDIR /ml\n",
      " ---> Running in 568e18cf0dae\n",
      "Removing intermediate container 568e18cf0dae\n",
      " ---> 551205df1e44\n",
      "Step 5/5 : ENTRYPOINT [\"python\", \"/ml/train_pix2pix.py\"]\n",
      " ---> Running in 94bb4ce9036c\n",
      "Removing intermediate container 94bb4ce9036c\n",
      " ---> c149f15231d8\n",
      "Successfully built c149f15231d8\n",
      "Successfully tagged gcr.io/wl-tex10-kfp-001/train_pix2pix:latest\n",
      "PUSH\n",
      "Pushing gcr.io/wl-tex10-kfp-001/train_pix2pix:latest\n",
      "The push refers to repository [gcr.io/wl-tex10-kfp-001/train_pix2pix]\n",
      "3896e6e64da4: Preparing\n",
      "aef30f407f82: Preparing\n",
      "a144de6e67e6: Preparing\n",
      "4b8ec9124f1c: Preparing\n",
      "652cdcb17d30: Preparing\n",
      "dd7f77c80a16: Preparing\n",
      "f4cb77175ac9: Preparing\n",
      "31835e84bcc0: Preparing\n",
      "75e70aa52609: Preparing\n",
      "dda151859818: Preparing\n",
      "fbd2732ad777: Preparing\n",
      "ba9de9d8475e: Preparing\n",
      "dd7f77c80a16: Waiting\n",
      "f4cb77175ac9: Waiting\n",
      "31835e84bcc0: Waiting\n",
      "75e70aa52609: Waiting\n",
      "dda151859818: Waiting\n",
      "fbd2732ad777: Waiting\n",
      "ba9de9d8475e: Waiting\n",
      "652cdcb17d30: Layer already exists\n",
      "4b8ec9124f1c: Layer already exists\n",
      "a144de6e67e6: Layer already exists\n",
      "f4cb77175ac9: Layer already exists\n",
      "dd7f77c80a16: Layer already exists\n",
      "31835e84bcc0: Layer already exists\n",
      "75e70aa52609: Layer already exists\n",
      "fbd2732ad777: Layer already exists\n",
      "dda151859818: Layer already exists\n",
      "ba9de9d8475e: Layer already exists\n",
      "3896e6e64da4: Pushed\n",
      "aef30f407f82: Pushed\n",
      "latest: digest: sha256:a7c1f66066ed0520b2808ca7c00f3b2d612056f80da26c8230bd418e7af8ceb2 size: 2832\n",
      "DONE\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "ID                                    CREATE_TIME                DURATION  SOURCE                                                                                      IMAGES                                           STATUS\n",
      "adc07b1a-85bd-419c-949a-1d5df624692e  2019-07-12T19:54:12+00:00  2M3S      gs://wl-tex10-kfp-001_cloudbuild/source/1562961228.22-d216aed74d1d4882a5cead76f238102a.tgz  gcr.io/wl-tex10-kfp-001/train_pix2pix (+1 more)  SUCCESS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Activated service account credentials for: [kubeflow-user@wl-tex10-kfp-001.iam.gserviceaccount.com]\n",
      "Creating temporary tarball archive of 1504 file(s) totalling 292.7 MiB before compression.\n",
      "Uploading tarball of [.] to [gs://wl-tex10-kfp-001_cloudbuild/source/1562961228.22-d216aed74d1d4882a5cead76f238102a.tgz]\n",
      "Created [https://cloudbuild.googleapis.com/v1/projects/wl-tex10-kfp-001/builds/adc07b1a-85bd-419c-949a-1d5df624692e].\n",
      "Logs are available at [https://console.cloud.google.com/gcr/builds/adc07b1a-85bd-419c-949a-1d5df624692e?project=612745165979].\n"
     ]
    }
   ],
   "source": [
    "%%bash -s \"$GCP_PROJECT_ID\"\n",
    "\n",
    "# -------------------------------------\n",
    "#     Define a Dockerfile, Build a  \n",
    "#       Docker image to package \n",
    "#   the Kubeflow Pipelines Operation \n",
    "# -------------------------------------\n",
    "\n",
    "cat > ./Dockerfile <<- \"EOF\"\n",
    "FROM tensorflow/tensorflow:1.14.0-py3\n",
    "RUN pip install --quiet tensorflow==2.0.0-beta1 Pillow \n",
    "ADD ./train_pix2pix.py /ml/train_pix2pix.py\n",
    "WORKDIR /ml\n",
    "\n",
    "ENTRYPOINT [\"python\", \"/ml/train_pix2pix.py\"]\n",
    "\n",
    "EOF\n",
    "\n",
    "# -------------------------------------\n",
    "#       Build a Docker Image on GCP \n",
    "#         using the gcloud tool\n",
    "# -------------------------------------\n",
    "\n",
    "gcloud auth activate-service-account --key-file=${GOOGLE_APPLICATION_CREDENTIALS}\n",
    "gcloud builds submit --tag gcr.io/$1/train_pix2pix:latest .\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Test the Pix2Pix model: Generate/Translate images from the Test dataset.\n",
    "\n",
    "**WIP** ADD GPU !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------- REMOTE BUILD OUTPUT ------------------------------\n",
      "starting build \"7a1d4396-a5fd-408e-bc39-358c01d4650a\"\n",
      "\n",
      "FETCHSOURCE\n",
      "Fetching storage object: gs://wl-tex10-kfp-001_cloudbuild/source/1562961378.86-214d7c4cbbeb4ed2b2e3ad18943336af.tgz#1562961402082950\n",
      "Copying gs://wl-tex10-kfp-001_cloudbuild/source/1562961378.86-214d7c4cbbeb4ed2b2e3ad18943336af.tgz#1562961402082950...\n",
      "/ [0 files][    0.0 B/178.3 MiB]                                                \r",
      "-\r",
      "- [0 files][ 51.1 MiB/178.3 MiB]                                                \r",
      "\\\r",
      "|\r",
      "| [0 files][149.0 MiB/178.3 MiB]                                                \r",
      "| [1 files][178.3 MiB/178.3 MiB]                                                \r\n",
      "Operation completed over 1 objects/178.3 MiB.                                    \n",
      "BUILD\n",
      "Already have image (with digest): gcr.io/cloud-builders/docker\n",
      "Sending build context to Docker daemon  308.8MB\r",
      "\r\n",
      "Step 1/5 : FROM tensorflow/tensorflow:1.14.0-py3\n",
      "1.14.0-py3: Pulling from tensorflow/tensorflow\n",
      "5b7339215d1d: Already exists\n",
      "14ca88e9f672: Already exists\n",
      "a31c3b1caad4: Already exists\n",
      "b054a26005b7: Already exists\n",
      "8832e3773578: Pulling fs layer\n",
      "5e671b828b2a: Pulling fs layer\n",
      "2b940936f993: Pulling fs layer\n",
      "016724bbd2c9: Pulling fs layer\n",
      "5bd1cb597025: Pulling fs layer\n",
      "68543864d644: Pulling fs layer\n",
      "016724bbd2c9: Waiting\n",
      "5bd1cb597025: Waiting\n",
      "68543864d644: Waiting\n",
      "2b940936f993: Verifying Checksum\n",
      "2b940936f993: Download complete\n",
      "5e671b828b2a: Verifying Checksum\n",
      "5e671b828b2a: Download complete\n",
      "5bd1cb597025: Verifying Checksum\n",
      "5bd1cb597025: Download complete\n",
      "68543864d644: Verifying Checksum\n",
      "68543864d644: Download complete\n",
      "8832e3773578: Verifying Checksum\n",
      "8832e3773578: Download complete\n",
      "016724bbd2c9: Verifying Checksum\n",
      "016724bbd2c9: Download complete\n",
      "8832e3773578: Pull complete\n",
      "5e671b828b2a: Pull complete\n",
      "2b940936f993: Pull complete\n",
      "016724bbd2c9: Pull complete\n",
      "5bd1cb597025: Pull complete\n",
      "68543864d644: Pull complete\n",
      "Digest: sha256:7c05dfab9ea82c95b571ca7dc3d92d4e0b289eeec6b3abf51f189caca0684488\n",
      "Status: Downloaded newer image for tensorflow/tensorflow:1.14.0-py3\n",
      " ---> 4cc892a3babd\n",
      "Step 2/5 : RUN pip install --quiet tensorflow==2.0.0-beta1 Pillow\n",
      " ---> Running in 40b7412b3c15\n",
      "Removing intermediate container 40b7412b3c15\n",
      " ---> f32d45880f58\n",
      "Step 3/5 : ADD ./test_pix2pix.py /ml/test_pix2pix.py\n",
      " ---> 06a8f5d19acc\n",
      "Step 4/5 : WORKDIR /ml\n",
      " ---> Running in db9d784b15e3\n",
      "Removing intermediate container db9d784b15e3\n",
      " ---> 51dea271c5f4\n",
      "Step 5/5 : ENTRYPOINT [\"python\", \"/ml/test_pix2pix.py\"]\n",
      " ---> Running in 2e37e823c931\n",
      "Removing intermediate container 2e37e823c931\n",
      " ---> 14ac5b84e40e\n",
      "Successfully built 14ac5b84e40e\n",
      "Successfully tagged gcr.io/wl-tex10-kfp-001/test_pix2pix:latest\n",
      "PUSH\n",
      "Pushing gcr.io/wl-tex10-kfp-001/test_pix2pix:latest\n",
      "The push refers to repository [gcr.io/wl-tex10-kfp-001/test_pix2pix]\n",
      "722331bb49aa: Preparing\n",
      "0ece7e008c37: Preparing\n",
      "a144de6e67e6: Preparing\n",
      "4b8ec9124f1c: Preparing\n",
      "652cdcb17d30: Preparing\n",
      "dd7f77c80a16: Preparing\n",
      "f4cb77175ac9: Preparing\n",
      "31835e84bcc0: Preparing\n",
      "75e70aa52609: Preparing\n",
      "dda151859818: Preparing\n",
      "fbd2732ad777: Preparing\n",
      "ba9de9d8475e: Preparing\n",
      "dd7f77c80a16: Waiting\n",
      "f4cb77175ac9: Waiting\n",
      "31835e84bcc0: Waiting\n",
      "75e70aa52609: Waiting\n",
      "dda151859818: Waiting\n",
      "fbd2732ad777: Waiting\n",
      "ba9de9d8475e: Waiting\n",
      "652cdcb17d30: Layer already exists\n",
      "4b8ec9124f1c: Layer already exists\n",
      "a144de6e67e6: Layer already exists\n",
      "dd7f77c80a16: Layer already exists\n",
      "f4cb77175ac9: Layer already exists\n",
      "31835e84bcc0: Layer already exists\n",
      "dda151859818: Layer already exists\n",
      "75e70aa52609: Layer already exists\n",
      "fbd2732ad777: Layer already exists\n",
      "ba9de9d8475e: Layer already exists\n",
      "722331bb49aa: Pushed\n",
      "0ece7e008c37: Pushed\n",
      "latest: digest: sha256:630632cb7a60df199f48d6dedc93dd81908a3b3bd6cb779118c484a2fedf3503 size: 2832\n",
      "DONE\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "ID                                    CREATE_TIME                DURATION  SOURCE                                                                                      IMAGES                                          STATUS\n",
      "7a1d4396-a5fd-408e-bc39-358c01d4650a  2019-07-12T19:56:42+00:00  2M1S      gs://wl-tex10-kfp-001_cloudbuild/source/1562961378.86-214d7c4cbbeb4ed2b2e3ad18943336af.tgz  gcr.io/wl-tex10-kfp-001/test_pix2pix (+1 more)  SUCCESS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Activated service account credentials for: [kubeflow-user@wl-tex10-kfp-001.iam.gserviceaccount.com]\n",
      "Creating temporary tarball archive of 1506 file(s) totalling 293.0 MiB before compression.\n",
      "Uploading tarball of [.] to [gs://wl-tex10-kfp-001_cloudbuild/source/1562961378.86-214d7c4cbbeb4ed2b2e3ad18943336af.tgz]\n",
      "Created [https://cloudbuild.googleapis.com/v1/projects/wl-tex10-kfp-001/builds/7a1d4396-a5fd-408e-bc39-358c01d4650a].\n",
      "Logs are available at [https://console.cloud.google.com/gcr/builds/7a1d4396-a5fd-408e-bc39-358c01d4650a?project=612745165979].\n"
     ]
    }
   ],
   "source": [
    "%%bash -s \"$GCP_PROJECT_ID\"\n",
    "\n",
    "# -------------------------------------\n",
    "#     Define a Dockerfile, Build a  \n",
    "#       Docker image to package \n",
    "#   the Kubeflow Pipelines Operation \n",
    "# -------------------------------------\n",
    "\n",
    "cat > ./Dockerfile <<- \"EOF\"\n",
    "FROM tensorflow/tensorflow:1.14.0-py3\n",
    "RUN pip install --quiet tensorflow==2.0.0-beta1 Pillow \n",
    "ADD ./test_pix2pix.py /ml/test_pix2pix.py\n",
    "WORKDIR /ml\n",
    "\n",
    "ENTRYPOINT [\"python\", \"/ml/test_pix2pix.py\"]\n",
    "\n",
    "EOF\n",
    "\n",
    "# -------------------------------------\n",
    "#       Build a Docker Image on GCP \n",
    "#         using the gcloud tool\n",
    "# -------------------------------------\n",
    "\n",
    "gcloud auth activate-service-account --key-file=${GOOGLE_APPLICATION_CREDENTIALS}\n",
    "gcloud builds submit --tag gcr.io/$1/test_pix2pix:latest .\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**WIP Investigate : kfp.compiler.build_docker_image ?**\n",
    "\n",
    "!gsutil cp ./download_dataset.py gs://wl-tex10-kfp-001/docker/download_dataset.py\n",
    "!gsutil cp ./Dockerfile gs://wl-tex10-kfp-001/docker/Dockerfile\n",
    "\n",
    "\n",
    "kfp.compiler.build_docker_image(staging_gcs_path='gs://wl-tex10-kfp-001/staging', \n",
    "                                target_image='gcr.io/{}/test:latest'.format(GCP_PROJECT_ID), \n",
    "                                dockerfile_path='gs://wl-tex10-kfp-001/docker/Dockerfile', \n",
    "                                timeout=600, namespace='kubeflow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### WIP : TEST A SINGLE COMPONENT PIPELINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------- \n",
    "# Create Kubeflow Pipelines Operations\n",
    "#        from Docker Images\n",
    "# -------------------------------------\n",
    "\n",
    "def download_op(fname=FILE_NAME, origin=URL, \n",
    "                cachedir=KERAS_CACHE_DIR, cachesubdir='datasets'):\n",
    "    \n",
    "    return kfp.dsl.ContainerOp(name=\"Download Dataset\",\n",
    "                               image='gcr.io/{}/download_dataset:latest'.format(GCP_PROJECT_ID),\n",
    "                               command=['python', '/ml/download_dataset.py'],\n",
    "                               arguments=['--fname', fname,\n",
    "                                          '--origin', origin,\n",
    "                                          '--cachedir', cachedir,\n",
    "                                          '--cachesubdir', cachesubdir ],\n",
    "                               file_outputs = {'outputdir': '/output.txt'}\n",
    "                              )\n",
    "\n",
    "\n",
    "def prepare_dataset_op(pathimg, pathimgsubdir, op_name):\n",
    "\n",
    "    return kfp.dsl.ContainerOp(name=op_name,\n",
    "                               image='gcr.io/{}/prepare_dataset:latest'.format(GCP_PROJECT_ID),\n",
    "                               command=['python', '/ml/prepare_dataset.py'],\n",
    "                               arguments=['--pathimg', pathimg,\n",
    "                                          '--pathimgsubdir', pathimgsubdir ],\n",
    "                               file_outputs = {'outputdir': '/output.txt'}\n",
    "                              )\n",
    "\n",
    "\n",
    "def train_op(pathdataset, epochs, pathmodel):\n",
    "\n",
    "    return kfp.dsl.ContainerOp(name=\"Train Pix2Pix model\",\n",
    "                               image='gcr.io/{}/train_pix2pix:latest'.format(GCP_PROJECT_ID),\n",
    "                               command=['python', '/ml/train_pix2pix.py'],\n",
    "                               arguments=['--dataset', pathdataset,\n",
    "                                          '--model', pathmodel,\n",
    "                                          '--epochs', epochs ],\n",
    "                               file_outputs = {'outputdir': '/output.txt'}\n",
    "                              )\n",
    "\n",
    "\n",
    "def test_op(pathdataset, pathoutput, pathmodel):\n",
    "\n",
    "    return kfp.dsl.ContainerOp(name=\"Test Pix2Pix\",\n",
    "                               image='gcr.io/{}/test_pix2pix:latest'.format(GCP_PROJECT_ID),\n",
    "                               command=['python', '/ml/test_pix2pix.py'],\n",
    "                               arguments=['--dataset', pathdataset,\n",
    "                                          '--output', pathoutput,\n",
    "                                          '--model', pathmodel ],\n",
    "                               file_outputs = {'outputdir': '/output.txt'}\n",
    "                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------- \n",
    "#  Build the pix2pix Pipeline Function\n",
    "# -------------------------------------\n",
    "\n",
    "@dsl.pipeline(\n",
    "    name='TEST pipeline',\n",
    "    description='A pipeline to download and prepare the dataset'\n",
    ")\n",
    "\n",
    "def pix2pix(\n",
    "    \n",
    "    ## -- Download Dataset Kubeflow Pipeline component parameters (with default values)\n",
    "    origin = dsl.PipelineParam('origin', value=URL),\n",
    "    fname = dsl.PipelineParam('fname', value=FILE_NAME),\n",
    "    cachedir = dsl.PipelineParam('cachedir', value=KERAS_CACHE_DIR), # on Kubeflow GKE/NFS \n",
    "    cachesubdir = dsl.PipelineParam('cachesubdir', value=\"datasets\"),\n",
    "    \n",
    "    ## -- Prepare Dataset Kubeflow Pipeline component parameters (with default values)\n",
    "    pathimgsubdirtrain = dsl.PipelineParam('pathimgsubdirtrain', value=\"train\"),\n",
    "    pathimgsubdirtest = dsl.PipelineParam('pathimgsubdirtest', value=\"test\"),\n",
    "    \n",
    "    ## -- Train Pix2Pix Kubeflow Pipeline component parameters (with default values)\n",
    "    epochs = dsl.PipelineParam('epochs', value=\"1\"),\n",
    "    model = dsl.PipelineParam('epochs', value=MODEL),\n",
    "    \n",
    "    ## -- Test Pix2Pix Kubeflow Pipeline component parameters (with default values)\n",
    "    output = dsl.PipelineParam('output', value=PIX2PIX_OUTPUTS)\n",
    "):\n",
    "    \n",
    "    \n",
    "    # Passing pipeline parameters as operation arguments (Returns a dsl.ContainerOp class instance)\n",
    "    download_task = download_op(fname, origin, cachedir, cachesubdir) \\\n",
    "                                .add_volume(k8s_client.V1Volume(name='workdir', \n",
    "                                                                persistent_volume_claim=k8s_client.V1PersistentVolumeClaimVolumeSource(claim_name='nfs'))) \\\n",
    "                                .add_volume_mount(k8s_client.V1VolumeMount(mount_path=NFS_MOUNT, name='workdir'))\n",
    "    \n",
    "    \n",
    "    prepare_train_task = prepare_dataset_op(pathimg=download_task.output, \n",
    "                                            pathimgsubdir=pathimgsubdirtrain,\n",
    "                                            op_name=\"Prepare Train Dataset\") \\\n",
    "                                .add_volume(k8s_client.V1Volume(name='workdir', \n",
    "                                                                persistent_volume_claim=k8s_client.V1PersistentVolumeClaimVolumeSource(claim_name='nfs'))) \\\n",
    "                                .add_volume_mount(k8s_client.V1VolumeMount(mount_path=NFS_MOUNT, name='workdir'))\n",
    "        \n",
    "\n",
    "    prepare_test_task = prepare_dataset_op(pathimg=download_task.output,\n",
    "                                           pathimgsubdir=pathimgsubdirtest,\n",
    "                                           op_name=\"Prepare test Dataset\") \\\n",
    "                                .add_volume(k8s_client.V1Volume(name='workdir', \n",
    "                                                                persistent_volume_claim=k8s_client.V1PersistentVolumeClaimVolumeSource(claim_name='nfs'))) \\\n",
    "                                .add_volume_mount(k8s_client.V1VolumeMount(mount_path=NFS_MOUNT, name='workdir'))\n",
    "    \n",
    "    ## WIP : add GPU\n",
    "    train_task = train_op(pathdataset=prepare_train_task.output,\n",
    "                          pathmodel=model,\n",
    "                          epochs=epochs) \\\n",
    "                                .add_volume(k8s_client.V1Volume(name='workdir', \n",
    "                                                                persistent_volume_claim=k8s_client.V1PersistentVolumeClaimVolumeSource(claim_name='nfs'))) \\\n",
    "                                .add_volume_mount(k8s_client.V1VolumeMount(mount_path=NFS_MOUNT, name='workdir'))\n",
    "    \n",
    "     ## WIP : add GPU\n",
    "    test_task = test_op(pathdataset=prepare_test_task.output, \n",
    "                        pathoutput=output,\n",
    "                        pathmodel=train_task.output) \\\n",
    "                                .add_volume(k8s_client.V1Volume(name='workdir', \n",
    "                                                                persistent_volume_claim=k8s_client.V1PersistentVolumeClaimVolumeSource(claim_name='nfs'))) \\\n",
    "                                .add_volume_mount(k8s_client.V1VolumeMount(mount_path=NFS_MOUNT, name='workdir'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------\n",
    "#       Compile the Pipeline \n",
    "# -------------------------------------\n",
    "\n",
    "pipeline_filename = pix2pix.__name__ + '.pipeline.tar.gz'\n",
    "compiler.Compiler().compile(pipeline_func=pix2pix, \n",
    "                            package_path=pipeline_filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Run link <a href=\"/pipeline/#/runs/details/75d379be-a4df-11e9-b468-42010a8400ef\" target=\"_blank\" >here</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ------------------------------------- \n",
    "#           Create (or reuse) a    \n",
    "#      Kubeflow Pipeline Experiment\n",
    "# -------------------------------------\n",
    "\n",
    "EXPERIMENT_NAME = \"TEST - Pix2Pix\"   ## Customize Name\n",
    "client = kfp.Client()\n",
    "\n",
    "try:\n",
    "    experiment = client.get_experiment(experiment_name=EXPERIMENT_NAME)\n",
    "except:\n",
    "    experiment = client.create_experiment(EXPERIMENT_NAME)\n",
    "\n",
    "# -------------------------------------\n",
    "#              Optional : \n",
    "# Specify/Overwrite pipeline arguments \n",
    "#     default values for execution\n",
    "# -------------------------------------\n",
    "\n",
    "#arguments = {'epochs': 1 }# Change to 200 for a full training \n",
    "\n",
    "# -------------------------------------\n",
    "#       Submit a pipeline run\n",
    "# -------------------------------------\n",
    "run_name = pix2pix.__name__ + ' run'\n",
    "run_result = client.run_pipeline(experiment.id, run_name, pipeline_filename)\n",
    "#run_result = client.run_pipeline(experiment.id, run_name, pipeline_filename, arguments)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls /mnt/nfs/data/datasets/facades\n",
    "!ls /mnt/nfs/data/models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%%bash\n",
    "\n",
    "rm -rf /mnt/nfs/data/*\n",
    "rm -f ./Dockerfile\n",
    "rm -f *.tar.gz"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "last_runtime": {
    "build_target": "//learning/brain/python/client:colab_notebook",
    "kind": "private"
   },
   "name": "pix2pix.ipynb",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
